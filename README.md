# ROC-AUC and KS Drift Monitoring Framework Comparison

| Metric | Description | Status | Drift Levels | Action Plan | Rationale for Thresholds |
|--------|-------------|---------|--------------|-------------|-------------------------|
| ROC-AUC Drift | Measures change in model's overall discriminatory power across all score ranges. Formula: (AUC_current - AUC_previous) / AUC_previous | ðŸ”´ Red | > 20% degradation | â€¢ Immediate escalation to governance committee<br>â€¢ Comprehensive root cause analysis<br>â€¢ Consider model redevelopment<br>â€¢ Increase monitoring frequency to weekly | â€¢ 20% represents ~2 standard deviations in AUC variation<br>â€¢ Significant impact on ranking ability<br>â€¢ Major deviation from expected model behavior<br>â€¢ High risk of incorrect decisions |
|  |  | ðŸŸ¡ Amber | 10-20% degradation | â€¢ Increase monitoring frequency to monthly<br>â€¢ Investigate population shifts<br>â€¢ Review recent decisions<br>â€¢ Document findings | â€¢ Represents 1-2 standard deviations<br>â€¢ Early warning zone before critical degradation<br>â€¢ Allows proactive investigation<br>â€¢ Typical impact: 5-15% on decisions |
|  |  | ðŸŸ¢ Green | â‰¤ 10% degradation | â€¢ Continue regular monitoring<br>â€¢ Document in quarterly report<br>â€¢ No immediate action needed | â€¢ Within 1 standard deviation<br>â€¢ Captures normal sampling variation (5-7%)<br>â€¢ Accounts for seasonal effects (3-5%)<br>â€¢ Cost of investigation exceeds benefits |
| KS Drift | Measures change in maximum separation between good/bad score distributions. Formula: (KS_current - KS_previous) / KS_previous | ðŸ”´ Red | > 20% degradation | â€¢ Same as ROC-AUC Red actions<br>â€¢ Additional focus on score distribution analysis<br>â€¢ Review score cutoff points | â€¢ 20% change indicates severe distribution shift<br>â€¢ Critical impact on separation ability<br>â€¢ High risk of overlap between good/bad populations<br>â€¢ Typically indicates structural model issues |
|  |  | ðŸŸ¡ Amber | 10-20% degradation | â€¢ Same as ROC-AUC Amber actions<br>â€¢ Additional focus on score bands near cutoff<br>â€¢ Analyze score migration patterns | â€¢ Significant shift in separation point<br>â€¢ May indicate population drift<br>â€¢ Risk of suboptimal cutoff points<br>â€¢ Usually requires score threshold review |
|  |  | ðŸŸ¢ Green | â‰¤ 10% degradation | â€¢ Same as ROC-AUC Green actions<br>â€¢ Monitor score distribution trends | â€¢ Normal variation in separation point<br>â€¢ Expected sampling variation (4-6%)<br>â€¢ Population stability maintained<br>â€¢ Acceptable business impact |

## Key Differences in Statistical Backing

### ROC-AUC Drift
1. Sampling Variation:
   - More stable across different sample sizes
   - Standard error typically 2-3% for 1000+ samples
   - Less sensitive to extreme values

2. Statistical Properties:
   - Global measure across all thresholds
   - More robust to population shifts
   - Better for overall discrimination assessment

### KS Drift
1. Sampling Variation:
   - More sensitive to sample size changes
   - Standard error typically 3-5% for 1000+ samples
   - More sensitive to extreme values

2. Statistical Properties:
   - Local measure at maximum separation point
   - More sensitive to population shifts
   - Better for specific cutoff point assessment

## Combined Monitoring Recommendation
- Monitor both metrics as they complement each other
- Use stricter threshold if metrics show different status levels
- KS drift may show problems before they appear in ROC-AUC
- ROC-AUC provides better overall stability assessment







Statistical backing: 20% often represents approximately 2 standard deviations in model performance variation
At this level, the model's predictive power has significantly declined
Industry practice often uses this as a critical threshold


Statistical backing:


Represents 1-2 standard deviations in typical model performance
Based on normal distribution properties:

~68% of variations fall within 1 standard deviation
~95% fall within 2 standard deviations


This range suggests meaningful drift that requires investigation but isn't yet critical



Represents natural variation within 1 standard deviation
In stable models, performance typically varies by 5-10% due to:

Sample variation
Seasonal effects
Normal population drift




From Baesens et al., "Analytics in a Big Data World" (2014):

Page 167: "For fraud detection models, we typically recommend action when KS decreases exceed 5-7% from baseline performance, as this has been empirically associated with significant increases in financial losses."




According to Baesens et al. (2014, p. 167, Analytics in a Big Data World), even minor decreases in the KS statisticâ€”approximately five to seven percentage points below baseline performanceâ€”have been empirically associated with significant increases in fraud losses, thereby warranting model recalibration.


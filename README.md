The chart below shows an improvement in performance for Version 4 when compared to the current model.
The AUC for the model's scorecards is acceptable as per industry standards, reflecting its effectiveness in rank-ordering predictions. The AUC represents the likelihood that a randomly selected good record has a higher score than a randomly selected bad record. A C-statistic of 0.5 describes a random model, whereas 1 describes a perfect model.

Note: The AUC statistics presented are based on development assumptions and have not been validated with current data.

KS Statistic:

Based on development assumptions, the KS statistic aligns with industry standards, suggesting the model effectively differentiates between goods and bads. KS measures the distance between the distributions of goods and bads captured by the score, and it is particularly relevant in risk industry decision-making.

Note: The KS statistic is based on development assumptions and has not been validated with current data.
The numbers referenced in the model document are provided here .However, the data needed to validate these numbers is not available, making it impossible to independently verify their accuracy or performance.
However, the data needed to validate these numbers is not available, making it impossible to independently verify their accuracy or performance.


The performance metric values such as AUC and KS mentioned above are taken from the model document. However, the data required to validate these metrics is not available, so their accuracy and reliability cannot be independently confirmed at this time."


Model overlays refer to adjustments made post-model development to ensure alignment with business requirements, practical constraints, or external factors not captured in the model. As part of the model overlays, a scaling transformation was applied to the log-odds values generated by each scorecard. This transformation was necessary to ensure consistency across scorecards, aligning all scores to a standardized range of 100â€“899. This approach ensures that records with the same score, irrespective of the scorecard used, are treated as equally risky. By standardizing the score range, the overlay enhances comparability and interpretability while maintaining the integrity of the risk assessment process.



